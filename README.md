# Outlier-Resistant A/B Test Evaluation Study


This project investigates the performance of three A/B testing methods under varying conditions of noise, outliers, and effect sizes. It systematically evaluates the **Standard t-test**, **Welch's t-test**, and a **10% Trimmed Mean t-test** across 100 different scenarios using a simulation framework in R.

---

## ğŸ“Œ Overview

A/B testing is widely used in industry to evaluate the impact of new features, products, or experiences. However, the statistical methods behind A/B testing often assume ideal conditionsâ€”equal variances, normality, and clean data. Real-world dataâ€”especially user behavior dataâ€”rarely satisfies these conditions.

In this study, we simulate **10,000 experiments per scenario** under combinations of:

- 5 **effect sizes**: `0`, `0.1`, `0.2`, `0.3`, `0.5`
- 4 **sample sizes per group**: `10`, `30`, `50`, `100`
- 5 **outlier levels**: `0%`, `2.5%`, `5%`, `10%`, `15%`

---

## ğŸ§ª Methodology

### Data Generation

- **Control group**: Normally distributed values with mean = 60, sd = 10
- **Treatment group**: Same base distribution with effect size shifts, variance changes, and injected high-value outliers (simulating long session times or bot behavior)

### Statistical Tests Compared

- **Standard t-test** â€“ assumes equal variances
- **Welchâ€™s t-test** â€“ accounts for unequal variances
- **Trimmed mean t-test (10%)** â€“ removes top and bottom 10% of each group

### Metrics Evaluated

- **Statistical power** across all conditions
- **Power differences** between tests
- **Visualizations**: power curves, boxplots, distribution plots

---

## ğŸ“Š Key Results

### Main Findings

- **Trimmed t-tests** outperform others when **outlier contamination is â‰¥10%**, especially at low effect sizes.
- **Welchâ€™s t-test** offers better protection when variances are unequal but can be slightly conservative.
- **Standard t-test** performs well under ideal conditions but suffers the most with outliers or heteroskedasticity.

### Model Performance & Visuals

- 100 simulated scenarios Ã— 10,000 repetitions
- Paired t-tests and effect size plots show significant and consistent trends
- Code saves summary tables and ggplot2 visualizations of results

## ğŸ“ Project Structure

ab_test_robustness/
â”‚
â”œâ”€â”€ FinalProjectCodeBehind.Rmd # R Markdown file with full simulation and analysis
â”œâ”€â”€ Clark-Enge---PSTAT-122-Final-Project.pdf # Final report (written analysis)
â”œâ”€â”€ results_df.rds # Saved simulation results (power values per scenario)
â”œâ”€â”€ diff_summary.rds # Paired t-test comparison table
â”œâ”€â”€ power_curves_es_tt.rds # Power curves by effect size
â”œâ”€â”€ power_curves_outlier.rds # Power curves by outlier %
â”œâ”€â”€ dist_differences.rds # Boxplot data for power differences between tests
â”œâ”€â”€ power_diff.rds # Boxplot data for power distributions
â”œâ”€â”€ README.md # Project documentation (this file)

yaml
Copy
Edit

---

## ğŸ“¦ Dependencies

This project was developed using R. To run the simulation and generate the report, ensure the following packages are installed:

### Required R Packages
```
â”œâ”€â”€ FinalProjectCodeBehind.Rmd # R Markdown file with full simulation and analysis
â”œâ”€â”€ Clark-Enge---PSTAT-122-Final-Project.pdf # Final report (written analysis)
â”œâ”€â”€ results_df.rds # Saved simulation results (power values per scenario)
â”œâ”€â”€ diff_summary.rds # Paired t-test comparison table
â”œâ”€â”€ power_curves_es_tt.rds # Power curves by effect size
â”œâ”€â”€ power_curves_outlier.rds # Power curves by outlier %
â”œâ”€â”€ dist_differences.rds # Boxplot data for power differences between tests
â”œâ”€â”€ power_diff.rds # Boxplot data for power distributions
â”œâ”€â”€ README.md # Project documentation (this file)
```
---

## How to Run
1. Clone the repository:
```
git clone https://github.com/yourusername/AB-Simulation-Study.git
cd AB-Simulation-Study
```
2. Open **AB-Simulation-Study.Rmd** in RStudio
3. Install required dependencies
4. Run the analysis by knitting the R Markdown file to a pdf
